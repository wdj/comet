##########################################
Overview:
This document contains notes on the 1-bit GEMM kernels and related changes made to CoMet. This includes additional input options, GEMM implementations, other implementation notes, and notes on planned changes.


##########################################
Input options:
--num_kernel <int> - Choose kernel number for routines that have multiple otpions
--print_details <yes/no> - Print additional details about each run


##########################################
GEMMs:
no tc:
  * CoMet GEMM kernels with original approach:
    0: Default Magma kernel (magma_wrapper.cc - magma_mgemm*5blas_zgemm)
    1: Simple WMMA CoMet kernel (tc_solve_comet.i.hh - b1_comet_gemm_gpu_simple)

tc 5:
  * 1-bit WMMA xor general GEMM kernels (tc_solve_general.i.hh):
    0: Very simple 1-bit GEMM (b1_xor_gemm_gpu)
      - Basic approach to develop CoMet 1-bit xor GEMM code
    1: Simple tensor core 1-bit GEMM (b1_xor_gemm_gpu_tc_simple)
      - Simple approach to demonstrate CoMet can use tensor cores
    2: Very simple shared memory 1-bit GEMM (b1_xor_gemm_gpu_tc_sm)
      - Simple approach to demonstrate tensor cores can use shared memory
      - Needs to read larger blocks and loop over them to actually improve performance
    30: Place holder to experiment with different pre and post processing routines

  * Cutlass device-level kernels (tc_solve_cutlass_general.i.hh):
    10: Cutlass kernel with 256x128 blocks
    11: Cutlass kernel with 128x256 blocks
      - Fastest Cutlass kernel on GTX 2060
    12: Cutlass kernel with 128x128 blocks
    13: Cutlass kernel with 128x64 blocks
    14: Cutlass kernel with 64x128 blocks
    15: Cutlass kernel with 64x64 blocks
    16: Cutlass WMMA kernel with 64x64 blocks
      - Another Cutlass kernel approach, but not sure what is different about it

  * 1-bit WMMA xor CoMet GEMM kernels (tc_solve_comet_xor.i.hh)
    20: Simple GEMM that returns int array (b1_comet_xor_gemm_gpu_int_simple)
      - Need to shuffle output array differently to get this to produce correct solution
    21: Simple xor GEMM (b1_comet_xor_gemm_gpu_simple)
      - Does not need tc_in or tc_out kernels
    22: Simple xor tensor core GEMM (b1_comet_xor_gemm_gpu_tc_simple)
      - Very simple working 1-bit tensor core GEMM
    23: In development Cutlass optimized xor tensor core GEMMs
      - In progress, unlikely to outperform Nvidia version
    24: Optimized Cutlass warp-level tensor core GEMMs from Nvidia
      - Fairly optimized Nvidia code with best performance
    25: Placeholder for experimenting with warp-level tensor core GEMM that outputs ints


##########################################
Implementation Notes:
In setup routines primarily use env.num_kernel()<20 for general GEMMs and env.num_kernel()>=20 for CoMet GEMMs to differentiate between the two types of GEMMs. Will likely pick fastest kernels later for master version of CoMet.

Have compile and run scripts. Run scripts have options to run accuracy tests and run performance tests. However this is not currently in repo.

Modified setup to automically install Cutlass from tar file and setup CoMet to use Cutlass.

Added test code to output input files to use for testing.

##########################################
Planned changes:
- Explore modifying warp-level Cutlass GEMM to use single mma call and store int result.
- Add additional performance analysis code for communication and i/o routines.

In progress:
- Add tests to driver_test.cc for various 1-bit and Cutlass kernels. Identify cases where they work correctly and when they do not, including checking corner cases.
  - Nvidia warp-level GEMM fails to produce correct solution, however running standalone tests says the result is correct
- Explore 3-way duo methods using 3-way scaling test as example
  - Test code doesn't actually call 3-way duo method - uses !is_duo for all calls

Potential changes:
- Explore approaches for developing faster tc_in/tc_out routines.

##########################################

