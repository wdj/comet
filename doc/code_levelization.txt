
Code levelization: subpackage dependencies
------------------------------------------

(ref.: John Lakos, Large-Scale C++ Software Design. Addison-Wesley, 1996)

[implementation highlights are in brackets]


0.   external libraries:
     CUDA
     ROCM
     CUBLAS
     ROCBLAS
     CUB
     HIPCUB
     CUTLASS
     MAGMA (patched)
          Patches to MAGMA to support needed modified GEMM operations
     MPI


1.   assertions.hh
     assertions.i.hh
     assertions.cc
          Macros and code for assertions
     types.hh
          Fundamental scalar types for algorithms; associated functions
          [types to support single or double build]
          [types to support 2x2 or 2x2x2 table]
     env.hh
     env.cc
          Basic environment - settings, MPI communicators, etc.
          [control of MPI proc grid axis order: vector, field, repl]
          [CoMet code proper does 1 phase/stage; driver loops to do multiple]


2a   utils.hh
          Miscellaneous math utilities
2b.  formulas.hh
          Basic formulas that define some of the metrics used.


3.   magma_wrapper.hh
     magma_wrapper.cc
          Interface to generalized linear algebra functions, e.g. MAGMA
          [because of MAGMABLAS limitations, it is needed to tile the
           GEMM to smaller sizes]
     mirrored_buf.hh
     mirrored_buf.cc
          Class to manage dual CPU/GPU reflected arrays
          [CPU/pinned and GPU mem, same size; user chooses when to transfer]


4.   compressed_buf.hh
     compressed_buf.i.hh
     compressed_buf.cc
          Similar to mirrored_buf but allows lossless compression of buffers.

5.   histogram.hh
     histogram.cc
          Manage histograms of metrics values.

6.   tc.hh
     tc.i.hh
     tc_helpers.i.hh
     tc_in.i.hh
     tc_solve.i.hh
     tc_out.i.hh
     tc.cc
          CUDA code, primarily for using tensor cores
          [operations: alloc/free buffers, copy in, GEMM, adjust output]
          [A and B are decomposed into block rows to conserve memory]
          [permutation is required to make each 2x2 table contig in memory]


7.   decomp_mgr.hh
     decomp_mgr.cc
          Define distribution of vectors to MPI ranks, padding needed, etc.
          [add padding vectors and fields to make same size per proc]
          [bitwise-field methods have notion of "packed field" (whole word)]


8.   linalg.hh
     linalg.i.hh
          Top level routines to perform actual or modified GEMMs.


9a.  vectors.hh
     vectors.cc
          Class to manage the set of vectors taken as input to the methods
          [need to set pad to zero since methods will compute on pad]
          [vector data can be either simple memory or pointer to mirrored buf]
          [can also copy vector memory to separate buf]
9b.  metrics_2way_indexing.i.hh
     metrics_2way_accessors.i.hh
     metrics_3way_indexing.i.hh
     metrics_3way_accessors.i.hh
     metrics.hh
     metrics.cc
          Class to manage the calculated metrics output from the methods
          [MetricsMem is reusable/resizable - used to avoid costly CUDA mallocs]
          [Czek stores num/denom together; CCC stored separately]
          [CCC requires sums "S" for denoms; CCC/sparse also needs counts "C"]
          [for some methods zeros matter, so "adjust" to remove pad effects]
          [2-way (3-way) method: produces 2D (3D) blocks]
          [2-way (3-way) method: vector_proc owns block row (slab of blocks)]
          [num_phase, num_proc_repl > 1 means skipped blocks, complicated code]
          [2-way: block active elts form triangle or square]
          [3-way: block active elts form tetrahedron, triang prism, or slice]
          [maps from local ij(k) and j_proc(k_proc) to metric index in memory]
          [maps from metric index in memory to (precomputed) global ij(k) coord]
          [_S_ (_C_) array stores packed 2 or 3 sums (counts)]
          [CCC methods compute num/denom quotient etc. in the accessor]
          [version to compute with int128 arith to ensure reproducibility]
          [version to compute threshold test quickly for speed]
          [3-way: SectionInfo struct to manage slice indexing]
          [3-way: ijk can be permuted for speed based on slice orientation]
          [3-way: index cache struct to reuse expensive index calcs]


10a. vector_sums.hh (depends on 8a)
     vector_sums.cc
          Compute the denominators needed by the methods
          [CCC: REF method computes 2-bits at a time, CPU whole word]
          [if num_proc_field > 1, need reduction]
10b. checksum.hh (depends on 8b)
     checksum.cc
          Utility to compute checksum of data in a metrics object
          [checksum per element is hashed-global-index times metric-value]
          ["locality-preserving" but bitwise accurate and decomp-independent]
          [multiprecision integers used]
10c. comm_xfer_utils.hh
     comm_xfer_utils.cc
          Utilities for communication and CPU/GPU transfer of vectors, metrics
          [if num_proc_field > 1, need reduction]


11a. compute_metrics_2way_block_nums.cc
          Calculate metrics numerators, 2-way, for a single block
          [non-all2all special case code: every block computes triangle]
     compute_metrics_2way_block_finalize.cc
          Combine numerators and denominators, 2-way, for a single block
          [2-way combine in separate function; 3-way: colocated with num calc]
          [2-way, 3-way CCC: no combine; just save sums and counts]
          [note metrics_buf is square, metrics is combined triang and squares]
     compute_metrics_2way_block.hh
          Declarations header file.
11b. compute_metrics_3way_block_nonlinalg.cc
          Calculate numerators, 3-way, for a single block, non-GPU case
          [not-all2all is always tetrahedron, otherwise tet, prism or slice]
     compute_metrics_3way_block_linalg.cc
          Calculate numerators, 3-way, for a single block, GPU case
          [steps in inner loop: form X, mGEMM on X, add result into metrics]
          [Czek requires 3 2-way matrices; CCC requires 3 inner mGEMM steps]
          [pipeline loop over planes X mGEMM-steps]
          [pipeline overlaps vecs-to-gpu, compute, metrics-from-gpu, reduce]
          [use "lock" mechanism to check for potential race conditions]
          [struct to keep track of current, next-step, lagged-step variables]
          ["stage" setting controls which planes in slice computed]
     compute_metrics_3way_block.hh
     compute_metrics_3way_block.cc
          Calculate numerators, 3-way, for a single block
          [allocate-once memory to reuse across calls, for speed]


12a. compute_metrics_2way.hh (depends on 10a)
     compute_metrics_2way.cc
          Calculate metrics, 2-way
          [pipeline loop over blocks]
          [vecs send/recv, vecs-to-gpu, compute, metrics-from-gpu, reduce]
          [use "lock" mechanism to check for potential race conditions]
          [struct to keep track of current, next-step, lagged-step variables]
          [two options for order of operations, for speed]
12b. compute_metrics_3way.hh (depends on 10b)
     compute_metrics_3way.cc
          Calculate metrics, 3-way
          [pipeline loops over blocks (or blocks X slices)]
          [pipeline overlap of vector send/recv, block (slice) compute]
          [main diag, plane and volume blocks]
          [1. main diag block loop 1 block (plane intersect main diag = point)]
          [2. plane block loop singly nested (diag intersect diag plane = line)]
          [3. volume block loop doubly nested loop (remaining blocks of slab)]
          [compute is lagged with respect to comm; spill from one loop to next]
          [num_phase and num_proc_repl may result in some steps skipped]


13.  compute_metrics.hh
     compute_metrics.cc
          Top-level function to calculate metrics


14.  vectors_io.hh
     vectors_io.cc
     metrics_io.hh
     metrics_io.cc
          Code to input vectors and write out metrics result
          [MetricsFile top-level class to write to metrics file]
          [MetricWriter helper class to write a single metric to metric file]
          [highly optimized functions for CCC 2-way and 3-way writing metrics]
          [CCC metrics write: special accessor to check threshold fast]
          [CCC metrics write: buffer output metrics that pass threshold; flush]
          [read vectors from file (Czek, CCC), according to file format]
          [function to write vectors to file, for testing purposes]
     test_problems.hh
     test_problems.cc
          Code to generate synthetic test problems
          [set vectors random: fully random; no known closed form solution]
          [set vectors analytic: less random, closed form solution for checking]
          [analytic: vectors block row with same entries down column; scramble]
     driver.hh
     driver.cc
          Driver to get options, generate or read input, calc and write metrics
          [loop over requested stages, phases]
          [use metrics_mem for reuse of metrics memory]


15.  genomics_metric.cc
          Main program
          [warmup code to deal with GPU that might be initially slow]
          [second warmup code to detect/mark slow nodes, map out]


(unit test modules)
          [most tests compare checksum for runs in different configurations]
          [some ground-truth checks of CCC against hand-written examples]

